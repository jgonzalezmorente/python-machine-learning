{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e12e159d-09ef-49c0-8889-6a50e882f04b",
   "metadata": {},
   "source": [
    "# Neuronas Lineales Adaptativas (ADALINE)\n",
    "\n",
    "El algoritmo **Adaline** (Adaptive Linear Neuron) es particularmente interesante porque ilustra conceptos fundamentales en el aprendizaje automático, como la definición y minimización de funciones de coste continuas. Estos conceptos son esenciales para entender algoritmos más avanzados de clasificación, como la **regresión logística**, las **máquinas de vectores de soporte** (SVM) y los modelos de regresión.\n",
    "\n",
    "## Diferencias entre Adaline y el Perceptrón\n",
    "\n",
    "Una diferencia clave entre **Adaline** y el **perceptrón de Rosenblatt** radica en cómo se actualizan los pesos. Mientras que el perceptrón utiliza una función de activación basada en un **escalón unitario**, Adaline utiliza una **función de activación lineal**. En Adaline, la actualización de los pesos se realiza después de evaluar todo el conjunto de datos, mientras que en el perceptrón, los pesos se actualizan tras cada muestra de entrenamiento. Este enfoque en Adaline se conoce como **descenso de gradiente en lotes**, ya que se ajustan los pesos considerando todas las muestras de una vez.\n",
    "\n",
    "En Adaline, la función de activación lineal, representada por $\\phi(z)$, es simplemente el producto escalar de los pesos y las entradas de la red:\n",
    "$$\n",
    "\\phi\\left(\\mathbf{w}^T\\mathbf{x}\\right) = \\mathbf{w}^T\\mathbf{x}\n",
    "$$\n",
    "Mientras que esta función de activación lineal se emplea para ajustar los pesos, el algoritmo sigue utilizando una **función umbral** para realizar la predicción final, que es similar a la función escalón del perceptrón. Es decir, aunque los pesos se ajustan basándose en valores continuos, la predicción final será una etiqueta de clase discreta (por ejemplo, +1 o -1), utilizando una función umbral que se aplica sobre la salida lineal.\n",
    "\n",
    "**Adaline** compara las etiquetas verdaderas de las clases con los valores continuos generados por la función de activación lineal para calcular el error. Esta es otra diferencia importante respecto al perceptrón, que compara directamente las etiquetas verdaderas con las etiquetas de clase predichas.\n",
    "\n",
    "## Minimizar la función de coste con el descenso del gradiente\n",
    "\n",
    "En Adaline, la función de coste se define como la **Suma de Errores Cuadráticos (SSE)** entre la salida calculada y la etiqueta de clase verdadera:\n",
    "$$\n",
    "J(\\mathbf{w}) = \\frac{1}{2} \\sum_i \\left( y^{(i)} - \\phi\\left(z^{(i)}\\right) \\right)^2\n",
    "$$\n",
    "Esta función de coste es convexa y diferenciable, lo que permite aplicar el algoritmo de **descenso del gradiente** para minimizarla. El objetivo es encontrar los pesos que minimicen esta función de coste ajustando los pesos actuales:\n",
    "$$\n",
    "\\mathbf{w} := \\mathbf{w} + \\Delta\\mathbf{w}\n",
    "$$\n",
    "donde $\\Delta\\mathbf{w}$ representa el cambio en los pesos, calculado como el **gradiente negativo** multiplicado por la tasa de aprendizaje $\\eta$:\n",
    "$$\n",
    "\\Delta\\mathbf{w} = -\\eta \\nabla J(\\mathbf{w})\n",
    "$$\n",
    "El gradiente de la función de coste se obtiene calculando la derivada parcial con respecto a cada peso $w_j$:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_j} = -\\sum_i \\left( y^{(i)} - \\phi\\left(z^{(i)}\\right) \\right) x_j^{(i)}\n",
    "$$\n",
    "Esto lleva a la expresión del gradiente:\n",
    "$$\n",
    "\\nabla J(\\mathbf{w}) = - \\left( \\mathbf{y} - \\phi(\\mathbf{z}) \\right)^T X\n",
    "$$\n",
    "donde $X$ es la matriz de datos de entrada, cuyas filas son las muestras de entrenamiento.\n",
    "\n",
    "Finalmente, el cambio en los pesos se expresa como:\n",
    "$$\n",
    "\\Delta\\mathbf{w} = \\eta \\left( \\mathbf{y} - \\phi(\\mathbf{z}) \\right)^T X\n",
    "$$\n",
    "y los pesos se actualizan con la fórmula:\n",
    "$$\n",
    "\\mathbf{w} := \\mathbf{w} + \\eta \\left( \\mathbf{y} - \\phi(\\mathbf{z}) \\right)^T X\n",
    "$$\n",
    "\n",
    "Aunque superficialmente la regla de aprendizaje de Adaline puede parecer similar a la del perceptrón, la diferencia radica en que $\\phi\\left(z^{(i)}\\right)$ es un valor continuo y no una etiqueta de clase categórica. Además, la actualización de los pesos en Adaline se realiza evaluando todo el conjunto de datos, mientras que en el perceptrón, los pesos se ajustan después de procesar cada muestra de manera incremental. Esto hace que Adaline utilice el enfoque de **descenso de gradiente en lotes**, una técnica más robusta que facilita la convergencia en problemas donde la función de coste es continua, diferenciable y convexa.\n",
    "\n",
    "## Resumen\n",
    "\n",
    "En resumen, **Adaline** constituye un importante paso hacia algoritmos más sofisticados de aprendizaje automático y permite el uso de métodos optimizados como el descenso del gradiente para ajustar los parámetros de los modelos de manera eficiente. La capacidad de definir y minimizar una función de coste diferenciable hace que Adaline sea una base fundamental para comprender los algoritmos más avanzados utilizados hoy en día en clasificación y regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5987e0-0862-43d9-9753-3ab22feebb80",
   "metadata": {},
   "source": [
    "### Implementación del algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a611d164-875b-4f6e-a8cd-cc4a1f394d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AdalineGD:\n",
    "    \"\"\" \n",
    "    AdalineGD (Adaline Gradiente Descendente).\n",
    "    Modelo de Adaline que utiliza el algoritmo de descenso del gradiente.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    eta : float, opcional, default=0.01\n",
    "        Tasa de aprendizaje (step size).\n",
    "    n_iter : int, opcional, default=50\n",
    "        Número de iteraciones (epochs) sobre el dataset de entrenamiento.\n",
    "    random_state : int, opcional, default=1\n",
    "        Semilla para la generación de números aleatorios para la inicialización de los pesos.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta  # Tasa de aprendizaje\n",
    "        self.n_iter = n_iter  # Número de iteraciones\n",
    "        self.random_state = random_state  # Semilla para reproducibilidad\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Ajusta el modelo a los datos de entrenamiento.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            Matriz con las características de entrenamiento.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Vector con las etiquetas de entrenamiento (objetivo).\n",
    "        \n",
    "        Devuelve:\n",
    "        ---------\n",
    "        self : objeto AdalineGD\n",
    "            Modelo entrenado.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generador de números aleatorios para inicializar los pesos de forma reproducible\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        # Inicialización de pesos pequeños cercanos a 0 (1 más que el número de características debido al término de sesgo)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "        self.cost_ = []  # Lista para almacenar los costos de cada iteración\n",
    "        \n",
    "        # Bucle de entrenamiento (descenso por gradiente)\n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)  # Cálculo de la entrada neta\n",
    "            output = self.activation(net_input)  # Cálculo de la salida del modelo\n",
    "            errors = (y - output)  # Cálculo del error entre la salida esperada y la real\n",
    "            # Actualización de los pesos: w_1, w_2, ..., w_m\n",
    "            self.w_[1:] += self.eta * X.T.dot(errors)\n",
    "            # Actualización del sesgo (w_0)\n",
    "            self.w_[0] += self.eta * errors.sum()\n",
    "            # Cálculo del coste basado en el error cuadrático medio\n",
    "            cost = (errors**2).sum() / 2.0\n",
    "            self.cost_.append(cost)  # Almacenar el coste de esta iteración\n",
    "        return self  # Devolver el modelo entrenado\n",
    "            \n",
    "    def net_input(self, X):\n",
    "        \"\"\"\n",
    "        Calcula la entrada neta.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            Matriz con las características.\n",
    "        \n",
    "        Devuelve:\n",
    "        ---------\n",
    "        net_input : float\n",
    "            Suma ponderada de las características más el sesgo.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Producto punto entre las características X y los pesos w_ (excepto w_0) más el sesgo w_0\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"\n",
    "        Función de activación lineal. En Adaline, la función de activación es simplemente la identidad.\n",
    "        \n",
    "        Parámetros:\n",
    "        X : float\n",
    "            Valor de entrada neta.\n",
    "        \n",
    "        Devuelve:\n",
    "        X : float\n",
    "            La salida sin cambios, ya que en Adaline la activación es una función identidad.\n",
    "        \"\"\"\n",
    "        return X  # La activación es lineal, así que se devuelve el valor de entrada neta sin cambios\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predice las etiquetas de las muestras dadas.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            Matriz con las características de prueba.\n",
    "        \n",
    "        Devuelve:\n",
    "        ---------\n",
    "        labels : array, shape = [n_samples]\n",
    "            Etiquetas predichas (1 o -1).\n",
    "        \"\"\"\n",
    "        # Predicción del valor basado en el signo de la entrada neta\n",
    "        # Si la entrada neta es >= 0, predice 1; si es < 0, predice -1\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b6a046-ff1a-4c3a-9964-edfe134b3910",
   "metadata": {},
   "source": [
    "Observa que el método `activation` no altera los resultados del modelo, ya que es una simple función identidad. Sin embargo, lo hemos incluido para ilustrar el flujo de información en una red neuronal de una sola capa. Este flujo sigue el proceso clásico: a partir de las características de los datos de entrada, se calcula la entrada neta, luego pasa por la activación y, finalmente, se genera la salida del modelo. Aunque en este caso la activación es lineal, más adelante veremos cómo el clasificador de regresión logística utiliza una función de activación no lineal. De hecho, el modelo de regresión logística y Adaline están estrechamente relacionados, diferenciándose principalmente en la función de activación y en la función de coste que emplean.\n",
    "\n",
    "De forma similar a la implementación del perceptrón, almacenamos los valores del coste en una lista llamada `self.cost_` para monitorizar el comportamiento del algoritmo durante el entrenamiento. Esta lista nos permite evaluar si el algoritmo está convergiendo adecuadamente, es decir, si los errores disminuyen de manera constante a lo largo de las iteraciones. Al analizar los valores almacenados en `self.cost_`, podemos determinar si el modelo ha alcanzado un mínimo en su función de coste, lo que indicaría que ha aprendido de manera efectiva a partir de los datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
